{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# UK Cyber Fraud Assistant - Fine-Tuning with Mistral-7B (Updated)\n\nThis notebook fine-tunes Mistral-7B-Instruct-v0.3 on UK cyber fraud guidance data using Unsloth for optimized training on Google Colab Pro A100.\n\n**Updates in v2:**\n- Increased dataset size from 111 to 278 QA pairs\n- Added early stopping to prevent overfitting\n\n## Setup and Installation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth and dependencies\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install transformers\n",
    "!pip install unsloth trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU setup\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Available VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import json\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "dataset_path = '/content/drive/MyDrive/Dissertation/cyber-fraud-chatbot/model_training/master_fraud_qa_dataset.json'\n",
    "\n",
    "# Load the fraud Q&A dataset\n",
    "with open(dataset_path, 'r') as f:\n",
    "    fraud_data = json.load(f)\n",
    "\n",
    "print(f\"Total samples: {len(fraud_data)}\")\n",
    "print(f\"Sample keys: {list(fraud_data[0].keys())}\")\n",
    "\n",
    "# Preview a sample\n",
    "sample = fraud_data[0]\n",
    "print(f\"\\nSample instruction: {sample['instruction']}\")\n",
    "print(f\"\\nSample output (first 200 chars): {sample['output'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data for instruction tuning with Mistral chat template\n",
    "def format_fraud_prompt(sample):\n",
    "    system_message = \"You are a helpful UK cyber fraud assistant providing empathetic support to fraud victims. Provide accurate, UK-specific guidance with proper contact numbers and procedures.\"\n",
    "\n",
    "    # Mistral chat format\n",
    "    formatted_text = f\"<s>[INST] {system_message}\\n\\n{sample['instruction']} [/INST] {sample['output']}</s>\"\n",
    "\n",
    "    return formatted_text\n",
    "\n",
    "# Apply formatting\n",
    "formatted_data = [format_fraud_prompt(item) for item in fraud_data]\n",
    "\n",
    "# Create train/validation split (80/20)\n",
    "split_idx = int(len(formatted_data) * 0.8)\n",
    "train_data = formatted_data[:split_idx]\n",
    "val_data = formatted_data[split_idx:]\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Dataset.from_dict({\"text\": train_data})\n",
    "val_dataset = Dataset.from_dict({\"text\": val_data})\n",
    "\n",
    "# Preview formatted sample\n",
    "print(f\"\\nFormatted sample (first 300 chars):\\n{formatted_data[0][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Full precision Mistral model\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"  # Original unquantized model\n",
    "max_seq_length = 2048\n",
    "dtype = torch.bfloat16  # Full precision\n",
    "\n",
    "# Load model without quantization\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=False,  # No quantization\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "print(\"Model loaded in full precision for LoRA training\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f}GB / {torch.cuda.max_memory_allocated()/1024**3:.2f}GB\")\n",
    "\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure LoRA for optimal fraud assistant training\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=32,  # Reduced rank for more stable training\n    target_modules=[\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\"\n    ],\n    lora_alpha=64,  # 2x rank for stable training\n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",  # Unsloth optimized checkpointing\n    random_state=3407,\n    use_rslora=False,\n    loftq_config=None,\n)\n\nprint(\"LoRA configuration applied\")\nmodel.print_trainable_parameters()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f}GB / {torch.cuda.max_memory_allocated()/1024**3:.2f}GB\")\n",
    "\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from trl import SFTTrainer\nfrom transformers import TrainingArguments, EarlyStoppingCallback\n\n# Training arguments\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=8,\n    warmup_steps=20,  # Increased for more gradual warmup\n    num_train_epochs=5,\n    learning_rate=5e-5,  # Reduced learning rate for stability\n    bf16=torch.cuda.is_bf16_supported(),\n    fp16=not torch.cuda.is_bf16_supported(),\n    logging_steps=5,\n    optim=\"adamw_torch\",                # Full precision optimizer\n    weight_decay=0.05,  # Increased weight decay for regularization\n    lr_scheduler_type=\"cosine\",\n    seed=3407,\n    output_dir=\"/content/drive/MyDrive/Dissertation/cyber-fraud-chatbot/trained_models\",\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    eval_strategy=\"epoch\",\n    evaluation_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    dataloader_pin_memory=True,\n    remove_unused_columns=False,\n    report_to=\"none\",\n)\n\nprint(\"Training arguments configured\")\nprint(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"Total training steps: {len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Move all model parameters to GPU before creating trainer\nmodel = model.to(\"cuda\")\n\n# Verify all parameters are on GPU\nprint(\"Checking model device placement...\")\nfor name, param in model.named_parameters():\n    if param.device.type == 'meta':\n        print(f\"Warning: {name} still on meta device\")\n    elif param.device.type != 'cuda':\n        print(f\"Moving {name} from {param.device} to cuda\")\n        param.data = param.data.to(\"cuda\")\n\nprint(\"All parameters moved to GPU\")\n\n# Initialize trainer with early stopping\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    args=training_args,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Increased patience\n)\n\nprint(\"Trainer initialized successfully with early stopping!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Final training loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable fast inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test scenarios for fraud assistant\n",
    "test_scenarios = [\n",
    "    \"I received a text saying my bank account is frozen and I need to pay Â£50 to unlock it. Is this legitimate?\",\n",
    "    \"Someone called claiming to be from HMRC saying I owe tax money. What should I do?\",\n",
    "    \"I paid for a loan arrangement fee but haven't received the loan. How can I get help?\",\n",
    "    \"How do I report a romance scam to the authorities?\",\n",
    "    \"Is there a way to check if an investment opportunity is legitimate?\"\n",
    "]\n",
    "\n",
    "def test_fraud_assistant(question):\n",
    "    system_message = \"You are a helpful UK cyber fraud assistant providing empathetic support to fraud victims. Provide accurate, UK-specific guidance with proper contact numbers and procedures.\"\n",
    "\n",
    "    # Format input using Mistral chat template\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"{system_message}\\n\\n{question}\"}\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    attention_mask = torch.ones_like(inputs)\n",
    "\n",
    "    # Generate response\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=512,\n",
    "        use_cache=True,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode response\n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Extract just the assistant's response\n",
    "    if \"[/INST]\" in response:\n",
    "        response = response.split(\"[/INST]\")[-1].strip()\n",
    "\n",
    "    if question in response:\n",
    "        response = response.split(question, 1)[-1].strip()\n",
    "\n",
    "    # Remove any remaining system instruction fragments\n",
    "    response = response.replace(\"Provide accurate, UK-specific guidance with proper contact numbers and procedures.\", \"\").strip()\n",
    "\n",
    "    # Clean up any leading punctuation or artifacts\n",
    "    while response.startswith((\".\", \"?\", \"!\", \":\")):\n",
    "        response = response[1:].strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "print(\"Testing fine-tuned fraud assistant:\\n\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test each scenario\n",
    "for i, scenario in enumerate(test_scenarios, 1):\n",
    "    print(f\"\\nTest {i}: {scenario}\\n\")\n",
    "    response = test_fraud_assistant(scenario)\n",
    "    print(f\"Assistant: {response}\\n\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model for Local Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save the trained adapter\nsave_path = \"/content/drive/MyDrive/Dissertation/cyber-fraud-chatbot/trained_models/uk-fraud-assistant-adapter\"\n\nmodel.save_pretrained(save_path)\ntokenizer.save_pretrained(save_path)\n\nprint(f\"Model adapter saved successfully to: {save_path}\")\n\n# Check if files were actually saved\nimport os\nif os.path.exists(save_path):\n    files = os.listdir(save_path)\n    print(f\"Files saved: {files}\")\nelse:\n    print(\"ERROR: Save path doesn't exist!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to GGUF format for local deployment\n",
    "gguf_save_path = \"/content/drive/MyDrive/Dissertation/cyber-fraud-chatbot/trained_models/uk-fraud-assistant-gguf\"\n",
    "\n",
    "model.save_pretrained_gguf(\n",
    "    gguf_save_path,\n",
    "    tokenizer,\n",
    "    quantization_method=\"q4_k_m\"  # Quantize only for deployment\n",
    ")\n",
    "\n",
    "print(f\"Model exported to GGUF format for local deployment at: {gguf_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Ollama Modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ollama Modelfile for easy deployment\n",
    "gguf_save_path = \"/content/drive/MyDrive/Dissertation/cyber-fraud-chatbot/trained_models/uk-fraud-assistant-gguf\"\n",
    "\n",
    "modelfile_content = '''FROM ./model-unsloth.Q4_K_M.gguf\n",
    "\n",
    "TEMPLATE \"\"\"<s>[INST] You are a helpful UK cyber fraud assistant providing empathetic\n",
    "support to fraud victims. Provide accurate, UK-specific guidance with proper contact\n",
    "numbers and procedures.\n",
    "\n",
    "{{ .Prompt }} [/INST] \"\"\"\n",
    "\n",
    "PARAMETER temperature 0.1\n",
    "PARAMETER top_p 0.9\n",
    "PARAMETER stop \"</s>\"\n",
    "PARAMETER stop \"[INST]\"\n",
    "PARAMETER stop \"[/INST]\"\n",
    "\n",
    "SYSTEM \"\"\"You are a specialized UK cyber fraud assistant. Your role is to:\n",
    "- Provide empathetic support to fraud victims\n",
    "- Offer accurate UK-specific guidance and procedures\n",
    "- Include proper UK contact numbers (Action Fraud: 0300 123 2040)\n",
    "- Maintain a supportive, non-judgmental tone\n",
    "- Help victims understand their next steps\n",
    "\"\"\"\n",
    "'''\n",
    "\n",
    "with open(f'{gguf_save_path}/Modelfile', 'w') as f:\n",
    "    f.write(modelfile_content)\n",
    "\n",
    "print(f\"Ollama Modelfile created at: {gguf_save_path}/Modelfile\")\n",
    "print(\"\\nTo deploy locally with Ollama:\")\n",
    "print(\"1. Download the uk-fraud-assistant-gguf folder from Google Drive\")\n",
    "print(\"2. cd uk-fraud-assistant-gguf\")\n",
    "print(\"3. ollama create uk-fraud-assistant -f Modelfile\")\n",
    "print(\"4. ollama run uk-fraud-assistant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Check if model files exist and find their actual location\nimport os\n\ndef check_model_files():\n    # Expected paths\n    gguf_path = \"/content/drive/MyDrive/Dissertation/cyber-fraud-chatbot/trained_models/uk-fraud-assistant-gguf\"\n    adapter_path = \"/content/drive/MyDrive/Dissertation/cyber-fraud-chatbot/trained_models/uk-fraud-assistant-adapter\"\n    \n    print(\"Checking expected paths...\")\n    print(f\"GGUF path exists: {os.path.exists(gguf_path)}\")\n    print(f\"Adapter path exists: {os.path.exists(adapter_path)}\")\n    \n    if os.path.exists(gguf_path):\n        print(f\"GGUF files: {os.listdir(gguf_path)}\")\n    \n    if os.path.exists(adapter_path):\n        print(f\"Adapter files: {os.listdir(adapter_path)}\")\n    \n    # If not found, search broader area\n    if not os.path.exists(gguf_path) or not os.path.exists(adapter_path):\n        print(\"\\nSearching for model files in Drive...\")\n        base_path = \"/content/drive/MyDrive\"\n        if os.path.exists(base_path):\n            for root, dirs, files in os.walk(base_path):\n                for file in files:\n                    if file.endswith(('.gguf', '.safetensors', '.bin')) and 'fraud' in root.lower():\n                        print(f\"Found model file: {os.path.join(root, file)}\")\n\ncheck_model_files()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create deployment zip with correct paths\nimport zipfile\nimport os\n\ndef create_deployment_zip():\n    gguf_path = \"/content/drive/MyDrive/Dissertation/cyber-fraud-chatbot/trained_models/uk-fraud-assistant-gguf\"\n    adapter_path = \"/content/drive/MyDrive/Dissertation/cyber-fraud-chatbot/trained_models/uk-fraud-assistant-adapter\"\n    \n    files_added = 0\n    with zipfile.ZipFile('uk-fraud-assistant-deployment.zip', 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Add GGUF files\n        if os.path.exists(gguf_path):\n            for file in os.listdir(gguf_path):\n                file_path = os.path.join(gguf_path, file)\n                if os.path.isfile(file_path):\n                    zipf.write(file_path, f\"uk-fraud-assistant-gguf/{file}\")\n                    print(f\"Added: uk-fraud-assistant-gguf/{file}\")\n                    files_added += 1\n        \n        # Add adapter files  \n        if os.path.exists(adapter_path):\n            for file in os.listdir(adapter_path):\n                file_path = os.path.join(adapter_path, file)\n                if os.path.isfile(file_path):\n                    zipf.write(file_path, f\"uk-fraud-assistant-adapter/{file}\")\n                    print(f\"Added: uk-fraud-assistant-adapter/{file}\")\n                    files_added += 1\n    \n    if files_added > 0:\n        print(f\"Zip created with {files_added} files\")\n        size = os.path.getsize('uk-fraud-assistant-deployment.zip') / 1024 / 1024\n        print(f\"Size: {size:.1f} MB\")\n        return True\n    else:\n        print(\"No files found!\")\n        return False\n\n# Create and download\nif create_deployment_zip():\n    from google.colab import files\n    files.download('uk-fraud-assistant-deployment.zip')\nelse:\n    print(\"Please check if the model files exist in the specified paths\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Training Summary and Next Steps\n\n### Key Improvements in v2:\n- **Dataset Size**: Increased from 111 to 278 QA pairs (2.5x increase)\n- **Early Stopping**: Added with patience=2 to prevent overfitting\n- **Conservative Parameters**: Kept original learning rate and epochs to prevent overfitting"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}