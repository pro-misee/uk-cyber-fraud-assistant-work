{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "# UK Cyber Fraud Assistant - Production Training (1000 Q&A Pairs)\n",
    "\n",
    "This notebook fine-tunes Mistral-7B-Instruct-v0.3 on the comprehensive 1000-pair UK cyber fraud guidance dataset using optimized parameters to prevent overfitting.\n",
    "\n",
    "**Key Improvements:**\n",
    "- 9x larger dataset (1000 vs 111 pairs) for robust training\n",
    "- Conservative hyperparameters to prevent overfitting\n",
    "- Early stopping and comprehensive monitoring\n",
    "- Hugging Face Hub integration for model sharing\n",
    "- Production-ready deployment artifacts\n",
    "\n",
    "## Dataset Composition (1000 Pairs)\n",
    "- **Action Fraud**: 265 pairs (fraud reporting guidance)\n",
    "- **CIFA**: 106 pairs (financial crime prevention)\n",
    "- **Which?**: 73 pairs (consumer protection)\n",
    "- **Take Five**: 89 pairs (banking fraud prevention)\n",
    "- **NCA**: 15 pairs (filtered consumer content)\n",
    "- **NCSC**: 10 pairs (social media safety)\n",
    "- **Romance Fraud**: 33 pairs (family protection)\n",
    "- **Gap Analysis**: 131 pairs (AI-enabled fraud, QR codes, recovery scams)\n",
    "- **Original Dataset**: 278 pairs (foundation content)\n",
    "\n",
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": [
    "# Install Unsloth and dependencies for optimized training\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install transformers\n",
    "!pip install unsloth[colab-new]@git+https://github.com/unslothai/unsloth.git\n",
    "!pip install trl peft accelerate bitsandbytes\n",
    "!pip install huggingface_hub wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify_setup"
   },
   "outputs": [],
   "source": [
    "# Verify GPU setup and memory\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear any previous GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Available VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB, Total: {total:.1f}GB\")\n",
    "\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset_section"
   },
   "source": [
    "## Load and Prepare 1000-Pair Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import json\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Path to the comprehensive 1000-pair dataset\n",
    "dataset_path = '/content/drive/MyDrive/Dissertation/uk-cyber-fraud-assistant/model_training/master_fraud_qa_dataset_1000_final.json'\n",
    "\n",
    "print(f\"Loading dataset from: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_dataset"
   },
   "outputs": [],
   "source": [
    "# Load the comprehensive fraud Q&A dataset\n",
    "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "    fraud_data = json.load(f)\n",
    "\n",
    "print(f\"Total samples loaded: {len(fraud_data)}\")\n",
    "print(f\"Sample keys: {list(fraud_data[0].keys())}\")\n",
    "\n",
    "# Analyze dataset composition\n",
    "if 'generated_by' in fraud_data[0]:\n",
    "    generation_sources = pd.Series([item.get('generated_by', 'unknown') for item in fraud_data])\n",
    "    print(f\"\\nGeneration sources:\")\n",
    "    print(generation_sources.value_counts())\n",
    "\n",
    "if 'scam_category' in fraud_data[0]:\n",
    "    scam_categories = pd.Series([item.get('scam_category', 'unknown') for item in fraud_data])\n",
    "    print(f\"\\nTop 10 scam categories:\")\n",
    "    print(scam_categories.value_counts().head(10))\n",
    "\n",
    "# Preview samples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE 1 (Action Fraud):\")\n",
    "sample1 = next((item for item in fraud_data if 'actionfraud' in item.get('source_url', '').lower()), fraud_data[0])\n",
    "print(f\"Instruction: {sample1['instruction']}\")\n",
    "print(f\"Output (first 150 chars): {sample1['output'][:150]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE 2 (Gap Analysis - AI Fraud):\")\n",
    "ai_sample = next((item for item in fraud_data if 'ai' in item.get('scam_category', '').lower() or 'voice' in item.get('instruction', '').lower()), fraud_data[1])\n",
    "print(f\"Instruction: {ai_sample['instruction']}\")\n",
    "print(f\"Output (first 150 chars): {ai_sample['output'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "format_data"
   },
   "outputs": [],
   "source": [
    "# Format data for instruction tuning with Mistral chat template\n",
    "def format_fraud_prompt(sample):\n",
    "    \"\"\"Format Q&A pair using Mistral-7B chat template\"\"\"\n",
    "    system_message = \"You are a helpful UK cyber fraud assistant providing empathetic support to fraud victims. Provide accurate, UK-specific guidance with proper contact numbers and procedures.\"\n",
    "    \n",
    "    # Handle both 'input' field variations\n",
    "    user_input = sample.get('input', '')\n",
    "    if user_input and user_input.strip():\n",
    "        full_instruction = f\"{sample['instruction']}\\n\\n{user_input}\"\n",
    "    else:\n",
    "        full_instruction = sample['instruction']\n",
    "    \n",
    "    # Mistral chat format with system message\n",
    "    formatted_text = f\"<s>[INST] {system_message}\\n\\n{full_instruction} [/INST] {sample['output']}</s>\"\n",
    "    \n",
    "    return formatted_text\n",
    "\n",
    "# Apply formatting to all samples\n",
    "print(\"Formatting dataset for training...\")\n",
    "formatted_data = [format_fraud_prompt(item) for item in fraud_data]\n",
    "\n",
    "# Create stratified train/validation split (80/20) to maintain category distribution\n",
    "# Use a more robust split for the larger dataset\n",
    "np.random.seed(42)\n",
    "indices = np.arange(len(formatted_data))\n",
    "train_indices, val_indices = train_test_split(\n",
    "    indices, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "train_data = [formatted_data[i] for i in train_indices]\n",
    "val_data = [formatted_data[i] for i in val_indices]\n",
    "\n",
    "print(f\"\\nDataset split:\")\n",
    "print(f\"Training samples: {len(train_data)} ({len(train_data)/len(formatted_data)*100:.1f}%)\")\n",
    "print(f\"Validation samples: {len(val_data)} ({len(val_data)/len(formatted_data)*100:.1f}%)\")\n",
    "print(f\"Total samples: {len(formatted_data)}\")\n",
    "\n",
    "# Create HuggingFace datasets\n",
    "train_dataset = Dataset.from_dict({\"text\": train_data})\n",
    "val_dataset = Dataset.from_dict({\"text\": val_data})\n",
    "\n",
    "# Preview formatted sample\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"FORMATTED SAMPLE (first 300 chars):\")\n",
    "print(formatted_data[0][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_section"
   },
   "source": [
    "## Load Model and Configure Optimized LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Model configuration - optimized for 1000-sample training\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "max_seq_length = 2048  # Sufficient for fraud guidance responses\n",
    "dtype = torch.bfloat16  # Full precision for maximum quality\n",
    "\n",
    "print(f\"Loading {model_name} in full precision...\")\n",
    "print(f\"Max sequence length: {max_seq_length}\")\n",
    "print(f\"Data type: {dtype}\")\n",
    "\n",
    "# Load model without quantization for full precision training\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=False,  # Full precision training\n",
    "    device_map={\"\":0},   # Single GPU mapping\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "print(\"\\nModel loaded successfully!\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "configure_lora"
   },
   "outputs": [],
   "source": [
    "# Configure LoRA with optimized parameters for 1000-sample training\n",
    "# Balanced approach: moderate rank reduction with slower learning rate\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=56,  # Balanced reduction: between 48-64 for optimal capacity vs overfitting prevention\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_alpha=112,  # 2x rank for stable training\n",
    "    lora_dropout=0,  # No dropout for Unsloth optimization\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Unsloth optimized checkpointing\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "print(\"LoRA configuration applied with optimized parameters:\")\n",
    "print(f\"Rank (r): 56 (balanced capacity for 1000-pair dataset)\")\n",
    "print(f\"Alpha: 112 (2x rank)\")\n",
    "print(f\"Dropout: 0 (Unsloth optimized)\")\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_section"
   },
   "source": [
    "## Configure Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_args"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "import os\n",
    "\n",
    "# Calculate optimal training steps\n",
    "per_device_batch_size = 2\n",
    "gradient_accumulation_steps = 8\n",
    "effective_batch_size = per_device_batch_size * gradient_accumulation_steps\n",
    "num_epochs = 5  \n",
    "\n",
    "steps_per_epoch = len(train_dataset) // effective_batch_size\n",
    "total_steps = steps_per_epoch * num_epochs\n",
    "warmup_steps = max(10, int(0.1 * steps_per_epoch))  # 10% of first epoch\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Per device batch size: {per_device_batch_size}\")\n",
    "print(f\"Gradient accumulation steps: {gradient_accumulation_steps}\")\n",
    "print(f\"Effective batch size: {effective_batch_size}\")\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Total epochs: {num_epochs}\")\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"Warmup steps: {warmup_steps}\")\n",
    "\n",
    "# Output directory\n",
    "output_dir = \"/content/drive/MyDrive/Dissertation/uk-cyber-fraud-assistant/trained_models/v3_1000_pairs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Training arguments with balanced settings for 1000-pair dataset\n",
    "training_args = TrainingArguments(\n",
    "    # Core training settings\n",
    "    per_device_train_batch_size=per_device_batch_size,\n",
    "    per_device_eval_batch_size=per_device_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=num_epochs,\n",
    "    \n",
    "    # Learning rate - conservative but sufficient for 5 epochs\n",
    "    learning_rate=2e-5,  # Conservative learning rate with more epochs for convergence\n",
    "    warmup_steps=warmup_steps,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    # Optimization settings\n",
    "    optim=\"adamw_torch\",  # Full precision optimizer\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,  # Gradient clipping\n",
    "    \n",
    "    # Precision settings\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    \n",
    "    # Evaluation and saving\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=max(1, steps_per_epoch // 2),  # Evaluate twice per epoch\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=max(1, steps_per_epoch // 2),\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Early stopping and best model\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=5,\n",
    "    logging_first_step=True,\n",
    "    \n",
    "    # Output\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    # Performance\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=2,\n",
    "    remove_unused_columns=False,\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=3407,\n",
    "    data_seed=3407,\n",
    "    \n",
    "    # Reporting\n",
    "    report_to=\"none\",  # enable wandb later\n",
    ")\n",
    "\n",
    "print(f\"\\nOutput directory: {output_dir}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"Evaluation strategy: {training_args.eval_strategy} every {training_args.eval_steps} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_trainer"
   },
   "outputs": [],
   "source": [
    "# Ensure model is on GPU\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "# Create early stopping callback to prevent overfitting\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,  # Stop if no improvement for 3 evaluations\n",
    "    early_stopping_threshold=0.001  # Minimum improvement threshold\n",
    ")\n",
    "\n",
    "# Initialize trainer with early stopping\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    args=training_args,\n",
    "    callbacks=[early_stopping],\n",
    "    packing=False,  # Don't pack sequences for better quality\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully with early stopping!\")\n",
    "print(f\"Early stopping patience: 3 evaluations\")\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_execution"
   },
   "source": [
    "## Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_model"
   },
   "outputs": [],
   "source": [
    "# Start training with comprehensive monitoring\n",
    "print(\"Starting training on 1000-pair dataset...\")\n",
    "print(\"Monitoring for overfitting with early stopping enabled.\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Clear GPU cache before training\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Execute training\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING COMPLETED!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Print comprehensive training statistics\n",
    "print(f\"Final training loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.1f} seconds\")\n",
    "print(f\"Samples per second: {trainer_stats.metrics['train_samples_per_second']:.2f}\")\n",
    "print(f\"Steps per second: {trainer_stats.metrics['train_steps_per_second']:.2f}\")\n",
    "\n",
    "# Get final evaluation results\n",
    "final_eval = trainer.evaluate()\n",
    "print(f\"\\nFinal validation loss: {final_eval['eval_loss']:.4f}\")\n",
    "print(f\"Final perplexity: {np.exp(final_eval['eval_loss']):.2f}\")\n",
    "\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_testing"
   },
   "source": [
    "## Test the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_model"
   },
   "outputs": [],
   "source": [
    "# Enable fast inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Comprehensive test scenarios covering the 1000-pair dataset scope\n",
    "test_scenarios = [\n",
    "    # Traditional fraud types\n",
    "    \"I received a text saying my bank account is frozen and I need to pay ¬£50 to unlock it. Is this legitimate?\",\n",
    "    \"Someone called claiming to be from HMRC saying I owe tax money. What should I do?\",\n",
    "    \"I paid for a loan arrangement fee but haven't received the loan. How can I get help?\",\n",
    "    \n",
    "    # AI-enabled fraud (new in 1000-pair dataset)\n",
    "    \"I received a call that sounded exactly like my daughter asking for money urgently. Could this be a scam?\",\n",
    "    \"Someone sent me a voice message that sounds like my boss asking me to transfer money. Is this possible fraud?\",\n",
    "    \n",
    "    # QR code fraud (new coverage)\n",
    "    \"I scanned a QR code for parking payment and now I'm worried it might have been fake. What should I do?\",\n",
    "    \n",
    "    # Romance fraud (enhanced coverage)\n",
    "    \"My elderly parent has been talking to someone online who is now asking for money. How can I help them?\",\n",
    "    \n",
    "    # Recovery fraud\n",
    "    \"Someone contacted me claiming they can recover money I lost to a scam, but they want an upfront fee. Is this legitimate?\",\n",
    "    \n",
    "    # Social media marketplace fraud\n",
    "    \"I'm trying to buy concert tickets from someone on Facebook. How can I avoid being scammed?\",\n",
    "    \n",
    "    # Action Fraud reporting\n",
    "    \"How do I report a fraud to Action Fraud and what information do I need?\"\n",
    "]\n",
    "\n",
    "def test_fraud_assistant(question):\n",
    "    \"\"\"Test the fine-tuned model with proper formatting\"\"\"\n",
    "    system_message = \"You are a helpful UK cyber fraud assistant providing empathetic support to fraud victims. Provide accurate, UK-specific guidance with proper contact numbers and procedures.\"\n",
    "    \n",
    "    # Format input using Mistral chat template\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"{system_message}\\n\\n{question}\"}\n",
    "    ]\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    attention_mask = torch.ones_like(inputs)\n",
    "    \n",
    "    # Generate response with optimized parameters\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=300,  # Adequate for comprehensive guidance\n",
    "            min_new_tokens=50,   # Ensure substantial responses\n",
    "            use_cache=True,\n",
    "            temperature=0.1,     # Low temperature for consistent guidance\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.05,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Extract assistant's response\n",
    "    if \"[/INST]\" in response:\n",
    "        response = response.split(\"[/INST]\")[-1].strip()\n",
    "    \n",
    "    # Clean up any artifacts\n",
    "    response = response.replace(\"Provide accurate, UK-specific guidance with proper contact numbers and procedures.\", \"\").strip()\n",
    "    \n",
    "    # Remove leading punctuation\n",
    "    while response.startswith((\".\", \"?\", \"!\", \":\", \",\")):\n",
    "        response = response[1:].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"Testing fine-tuned UK Cyber Fraud Assistant (1000-pair model)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_tests"
   },
   "outputs": [],
   "source": [
    "# Test each scenario\n",
    "for i, scenario in enumerate(test_scenarios, 1):\n",
    "    print(f\"\\nTest {i}: {scenario}\\n\")\n",
    "    response = test_fraud_assistant(scenario)\n",
    "    print(f\"Assistant: {response}\\n\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL TESTING COMPLETED\")\n",
    "print(\"Check responses for:\")\n",
    "print(\"‚úì UK-specific contact numbers (Action Fraud: 0300 123 2040)\")\n",
    "print(\"‚úì Empathetic, supportive tone\")\n",
    "print(\"‚úì Practical, actionable guidance\")\n",
    "print(\"‚úì Coverage of new fraud types (AI, QR codes, recovery scams)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_model"
   },
   "source": [
    "## Save Model and Create Deployment Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_adapter"
   },
   "outputs": [],
   "source": [
    "# Save the trained adapter\n",
    "adapter_save_path = f\"{output_dir}/uk-fraud-assistant-adapter\"\n",
    "model.save_pretrained(adapter_save_path)\n",
    "tokenizer.save_pretrained(adapter_save_path)\n",
    "\n",
    "print(f\"Model adapter saved to: {adapter_save_path}\")\n",
    "\n",
    "# Save training metadata\n",
    "training_metadata = {\n",
    "    \"model_name\": model_name,\n",
    "    \"dataset_size\": len(fraud_data),\n",
    "    \"train_samples\": len(train_dataset),\n",
    "    \"val_samples\": len(val_dataset),\n",
    "    \"final_train_loss\": float(trainer_stats.training_loss),\n",
    "    \"final_eval_loss\": float(final_eval['eval_loss']),\n",
    "    \"training_time_seconds\": float(trainer_stats.metrics['train_runtime']),\n",
    "    \"learning_rate\": float(training_args.learning_rate),\n",
    "    \"lora_rank\": 56,\n",
    "    \"lora_alpha\": 112,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"effective_batch_size\": effective_batch_size,\n",
    "    \"max_seq_length\": max_seq_length,\n",
    "    \"early_stopping_used\": True,\n",
    "    \"dataset_composition\": {\n",
    "        \"action_fraud\": 265,\n",
    "        \"cifa\": 106,\n",
    "        \"which\": 73,\n",
    "        \"take_five\": 89,\n",
    "        \"nca_filtered\": 15,\n",
    "        \"ncsc_filtered\": 10,\n",
    "        \"romance_fraud\": 33,\n",
    "        \"gap_analysis\": 131,\n",
    "        \"original_dataset\": 278\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{adapter_save_path}/training_metadata.json\", 'w') as f:\n",
    "    json.dump(training_metadata, f, indent=2)\n",
    "\n",
    "print(f\"Training metadata saved to: {adapter_save_path}/training_metadata.json\")\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export_gguf"
   },
   "outputs": [],
   "source": [
    "# Export to GGUF format for local deployment\n",
    "gguf_save_path = f\"{output_dir}/uk-fraud-assistant-gguf\"\n",
    "\n",
    "print(\"Exporting to GGUF format for local deployment...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "model.save_pretrained_gguf(\n",
    "    gguf_save_path,\n",
    "    tokenizer,\n",
    "    quantization_method=\"q4_k_m\",  # Optimal quality-size balance\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "print(f\"\\nGGUF model exported to: {gguf_save_path}\")\n",
    "\n",
    "# Create Ollama Modelfile\n",
    "modelfile_content = '''FROM ./model-unsloth.Q4_K_M.gguf\n",
    "\n",
    "TEMPLATE \"\"\"<s>[INST] You are a helpful UK cyber fraud assistant providing empathetic\n",
    "support to fraud victims. Provide accurate, UK-specific guidance with proper contact\n",
    "numbers and procedures.\n",
    "\n",
    "{{ .Prompt }} [/INST] \"\"\"\n",
    "\n",
    "PARAMETER temperature 0.1\n",
    "PARAMETER top_p 0.9\n",
    "PARAMETER stop \"</s>\"\n",
    "PARAMETER stop \"[INST]\"\n",
    "PARAMETER stop \"[/INST]\"\n",
    "\n",
    "SYSTEM \"\"\"You are a specialized UK cyber fraud assistant trained on 1000 Q&A pairs from authoritative sources. Your role is to:\n",
    "- Provide empathetic support to fraud victims\n",
    "- Offer accurate UK-specific guidance and procedures\n",
    "- Include proper UK contact numbers (Action Fraud: 0300 123 2040)\n",
    "- Maintain a supportive, non-judgmental tone\n",
    "- Help victims understand their next steps\n",
    "- Cover emerging fraud types including AI-enabled scams, QR code fraud, and recovery scams\n",
    "\"\"\"\n",
    "'''\n",
    "\n",
    "with open(f'{gguf_save_path}/Modelfile', 'w') as f:\n",
    "    f.write(modelfile_content)\n",
    "\n",
    "print(f\"Ollama Modelfile created at: {gguf_save_path}/Modelfile\")\n",
    "\n",
    "# Create deployment instructions\n",
    "deployment_instructions = '''# UK Cyber Fraud Assistant - Deployment Guide\n",
    "\n",
    "## Model Information\n",
    "- Base Model: Mistral-7B-Instruct-v0.3\n",
    "- Training Dataset: 1000 UK fraud guidance Q&A pairs\n",
    "- Sources: Action Fraud, CIFA, Which?, Take Five, NCA, NCSC, and gap analysis\n",
    "- Quantization: Q4_K_M (optimal quality-size balance)\n",
    "\n",
    "## Local Deployment with Ollama\n",
    "\n",
    "1. Install Ollama:\n",
    "   ```bash\n",
    "   curl -fsSL https://ollama.ai/install.sh | sh\n",
    "   ```\n",
    "\n",
    "2. Deploy the model:\n",
    "   ```bash\n",
    "   cd uk-fraud-assistant-gguf\n",
    "   ollama create uk-fraud-assistant -f Modelfile\n",
    "   ollama run uk-fraud-assistant\n",
    "   ```\n",
    "\n",
    "## Local Deployment with LM Studio\n",
    "\n",
    "1. Download and install LM Studio\n",
    "2. Load the model-unsloth.Q4_K_M.gguf file\n",
    "3. Configure system prompt:\n",
    "   \"You are a helpful UK cyber fraud assistant providing empathetic support to fraud victims. Provide accurate, UK-specific guidance with proper contact numbers and procedures.\"\n",
    "4. Set parameters:\n",
    "   - Temperature: 0.1\n",
    "   - Top-p: 0.9\n",
    "   - Max tokens: 350\n",
    "\n",
    "## Model Capabilities\n",
    "\n",
    "- Traditional fraud types (phishing, vishing, romance scams)\n",
    "- AI-enabled fraud (voice cloning, deepfakes)\n",
    "- QR code fraud and quishing\n",
    "- Recovery scams and advance fee fraud\n",
    "- Social media marketplace fraud\n",
    "- UK-specific reporting procedures\n",
    "- Empathetic victim support\n",
    "\n",
    "## Important Notes\n",
    "\n",
    "- This model provides guidance based on UK fraud prevention sources\n",
    "- Always encourage users to report fraud to Action Fraud (0300 123 2040)\n",
    "- For immediate emergencies, direct users to contact police (999)\n",
    "- The model is trained for supportive guidance, not as a replacement for professional advice\n",
    "'''\n",
    "\n",
    "with open(f'{gguf_save_path}/DEPLOYMENT.md', 'w') as f:\n",
    "    f.write(deployment_instructions)\n",
    "\n",
    "print(f\"Deployment instructions created at: {gguf_save_path}/DEPLOYMENT.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huggingface_upload"
   },
   "source": [
    "## Upload to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_hf"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login, HfApi, create_repo\n",
    "import os\n",
    "\n",
    "# Login to Hugging Face (you'll need to provide your token)\n",
    "print(\"Please enter your Hugging Face token:\")\n",
    "print(\"You can find your token at: https://huggingface.co/settings/tokens\")\n",
    "\n",
    "# Uncomment and use this for automated login (replace with your token)\n",
    "# hf_token = \"your_huggingface_token_here\"\n",
    "# login(token=hf_token)\n",
    "\n",
    "# Or use interactive login\n",
    "login()\n",
    "\n",
    "# Repository configuration\n",
    "repo_id = \"your-username/uk-cyber-fraud-assistant-mistral-7b\"  # Change to your username\n",
    "print(f\"Repository ID: {repo_id}\")\n",
    "\n",
    "# Create repository if it doesn't exist\n",
    "api = HfApi()\n",
    "try:\n",
    "    create_repo(\n",
    "        repo_id=repo_id,\n",
    "        private=False,  # Set to True if you want a private repository\n",
    "        repo_type=\"model\"\n",
    "    )\n",
    "    print(f\"Repository created: https://huggingface.co/{repo_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Repository might already exist or error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_model"
   },
   "outputs": [],
   "source": [
    "# Create model card\n",
    "model_card_content = f'''---\n",
    "license: apache-2.0\n",
    "base_model: mistralai/Mistral-7B-Instruct-v0.3\n",
    "tags:\n",
    "- fraud-detection\n",
    "- cybersecurity\n",
    "- uk-specific\n",
    "- victim-support\n",
    "- unsloth\n",
    "- mistral\n",
    "language:\n",
    "- en\n",
    "metrics:\n",
    "- perplexity\n",
    "library_name: transformers\n",
    "pipeline_tag: text-generation\n",
    "---\n",
    "\n",
    "# UK Cyber Fraud Assistant - Mistral-7B\n",
    "\n",
    "## Model Description\n",
    "\n",
    "This model is a fine-tuned version of Mistral-7B-Instruct-v0.3 specifically designed to provide empathetic support and accurate guidance to UK cyber fraud victims. It has been trained on 1000 high-quality Q&A pairs derived from authoritative UK fraud prevention sources.\n",
    "\n",
    "## Training Data\n",
    "\n",
    "The model was trained on a comprehensive dataset of 1000 Q&A pairs from:\n",
    "- **Action Fraud** (265 pairs) - UK's national fraud reporting centre\n",
    "- **CIFA** (106 pairs) - Fraud prevention organisation\n",
    "- **Which?** (73 pairs) - Consumer protection organisation\n",
    "- **Take Five** (89 pairs) - Banking fraud prevention campaign\n",
    "- **National Crime Agency** (15 pairs) - Consumer-relevant content\n",
    "- **NCSC** (10 pairs) - Social media safety guidance\n",
    "- **Romance Fraud Content** (33 pairs) - Family protection strategies\n",
    "- **Gap Analysis Content** (131 pairs) - AI-enabled fraud, QR codes, recovery scams\n",
    "- **Original Dataset** (278 pairs) - Foundation fraud guidance\n",
    "\n",
    "## Model Capabilities\n",
    "\n",
    "- Provides UK-specific fraud guidance with correct contact numbers\n",
    "- Covers traditional fraud types (phishing, vishing, romance scams)\n",
    "- Addresses emerging threats (AI-enabled fraud, QR code scams, recovery fraud)\n",
    "- Maintains empathetic, supportive tone for fraud victims\n",
    "- Includes proper reporting procedures for UK authorities\n",
    "\n",
    "## Training Details\n",
    "\n",
    "- **Base Model**: mistralai/Mistral-7B-Instruct-v0.3\n",
    "- **Training Method**: Full-precision LoRA with Unsloth optimization\n",
    "- **LoRA Rank**: 56 (optimized for 1000-pair dataset)\n",
    "- **LoRA Alpha**: 112 (2x rank for stable training)\n",
    "- **Learning Rate**: 2e-5 (conservative for robust convergence)\n",
    "- **Epochs**: 5 (optimal for dataset size and learning rate)\n",
    "- **Training Dataset**: 1000 samples (800 train, 200 validation)\n",
    "- **Early Stopping**: Enabled with patience=3\n",
    "- **Final Training Loss**: {trainer_stats.training_loss:.4f}\n",
    "- **Final Validation Loss**: {final_eval['eval_loss']:.4f}\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"{repo_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{repo_id}\")\n",
    "\n",
    "# Format input\n",
    "system_message = \"You are a helpful UK cyber fraud assistant providing empathetic support to fraud victims. Provide accurate, UK-specific guidance with proper contact numbers and procedures.\"\n",
    "user_question = \"I think I've been scammed. What should I do?\"\n",
    "\n",
    "messages = [{{\"role\": \"user\", \"content\": f\"{{system_message}}\\\\n\\\\n{{user_question}}\"}}]\n",
    "inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "\n",
    "# Generate response\n",
    "outputs = model.generate(inputs, max_new_tokens=300, temperature=0.1, do_sample=True, top_p=0.9)\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "```\n",
    "\n",
    "## Local Deployment\n",
    "\n",
    "For local deployment, use the GGUF quantized version with Ollama or LM Studio:\n",
    "\n",
    "### Ollama\n",
    "```bash\n",
    "ollama create uk-fraud-assistant -f Modelfile\n",
    "ollama run uk-fraud-assistant\n",
    "```\n",
    "\n",
    "### LM Studio\n",
    "Load the GGUF file with:\n",
    "- Temperature: 0.1\n",
    "- Top-p: 0.9\n",
    "- Max tokens: 350\n",
    "\n",
    "## Important Notes\n",
    "\n",
    "- This model provides guidance based on UK fraud prevention sources\n",
    "- Always encourage users to report fraud to Action Fraud (0300 123 2040)\n",
    "- For emergencies, direct users to contact police (999)\n",
    "- This model is for supportive guidance, not professional legal advice\n",
    "\n",
    "## License\n",
    "\n",
    "Apache 2.0 - See base model license for details.\n",
    "\n",
    "## Citation\n",
    "\n",
    "If you use this model, please cite:\n",
    "\n",
    "```\n",
    "@model{{uk-cyber-fraud-assistant-mistral-7b,\n",
    "  title={{UK Cyber Fraud Assistant - Mistral-7B}},\n",
    "  author={{Your Name}},\n",
    "  year={{2025}},\n",
    "  url={{https://huggingface.co/{repo_id}}}\n",
    "}}\n",
    "```\n",
    "'''\n",
    "\n",
    "# Save model card\n",
    "with open(f\"{adapter_save_path}/README.md\", 'w') as f:\n",
    "    f.write(model_card_content)\n",
    "\n",
    "print(\"Model card created\")\n",
    "\n",
    "# Upload the model to Hugging Face\n",
    "print(\"Uploading model to Hugging Face Hub...\")\n",
    "print(\"This may take several minutes depending on your internet connection.\")\n",
    "\n",
    "model.push_to_hub(\n",
    "    repo_id,\n",
    "    token=True,  # Use the token from login\n",
    "    commit_message=f\"Upload UK Cyber Fraud Assistant (1000 pairs, val_loss: {final_eval['eval_loss']:.4f})\"\n",
    ")\n",
    "\n",
    "tokenizer.push_to_hub(\n",
    "    repo_id,\n",
    "    token=True,\n",
    "    commit_message=\"Upload tokenizer\"\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Model successfully uploaded to: https://huggingface.co/{repo_id}\")\n",
    "print(f\"‚úÖ Repository includes model, tokenizer, and documentation\")\n",
    "print(f\"‚úÖ Model card with training details and usage instructions included\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_section"
   },
   "source": [
    "## Create Download Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_package"
   },
   "outputs": [],
   "source": [
    "# Create comprehensive deployment package\n",
    "import zipfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def create_deployment_package():\n",
    "    package_name = \"uk-fraud-assistant-v3-1000pairs-deployment.zip\"\n",
    "    \n",
    "    print(\"Creating comprehensive deployment package...\")\n",
    "    \n",
    "    with zipfile.ZipFile(package_name, 'w', zipfile.ZIP_DEFLATED, compresslevel=6) as zipf:\n",
    "        # Add GGUF files for local deployment\n",
    "        gguf_path = Path(gguf_save_path)\n",
    "        if gguf_path.exists():\n",
    "            for file_path in gguf_path.rglob('*'):\n",
    "                if file_path.is_file():\n",
    "                    arcname = f\"gguf/{file_path.relative_to(gguf_path)}\"\n",
    "                    zipf.write(str(file_path), arcname)\n",
    "                    \n",
    "        # Add adapter files for advanced users\n",
    "        adapter_path = Path(adapter_save_path)\n",
    "        if adapter_path.exists():\n",
    "            for file_path in adapter_path.rglob('*'):\n",
    "                if file_path.is_file() and file_path.suffix in ['.json', '.bin', '.safetensors', '.md']:\n",
    "                    arcname = f\"adapter/{file_path.relative_to(adapter_path)}\"\n",
    "                    zipf.write(str(file_path), arcname)\n",
    "    \n",
    "    file_size = os.path.getsize(package_name) / (1024 * 1024)\n",
    "    print(f\"\\nDeployment package created: {package_name}\")\n",
    "    print(f\"Package size: {file_size:.1f} MB\")\n",
    "    \n",
    "    # Create deployment summary\n",
    "    summary = f\"\"\"# UK Cyber Fraud Assistant V3 - Deployment Package\n",
    "\n",
    "## Package Contents\n",
    "- **GGUF Model**: Quantized model for local deployment (Ollama/LM Studio)\n",
    "- **Adapter Files**: LoRA adapter for advanced users\n",
    "- **Modelfile**: Ollama configuration\n",
    "- **Documentation**: Deployment instructions and model information\n",
    "\n",
    "## Model Performance\n",
    "- Training Dataset: 1000 high-quality Q&A pairs\n",
    "- Final Training Loss: {trainer_stats.training_loss:.4f}\n",
    "- Final Validation Loss: {final_eval['eval_loss']:.4f}\n",
    "- Training Time: {trainer_stats.metrics['train_runtime']:.1f} seconds\n",
    "- No Overfitting: Early stopping enabled\n",
    "\n",
    "## Quick Start\n",
    "1. Extract the deployment package\n",
    "2. Follow instructions in gguf/DEPLOYMENT.md\n",
    "3. Deploy with Ollama or LM Studio\n",
    "\n",
    "## Support\n",
    "- Covers all major fraud types including AI-enabled scams\n",
    "- UK-specific guidance with proper contact numbers\n",
    "- Empathetic support for fraud victims\n",
    "- Trained on authoritative sources (Action Fraud, CIFA, Which?, etc.)\n",
    "\n",
    "Package created: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Total package size: {file_size:.1f} MB\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(\"PACKAGE_SUMMARY.md\", 'w') as f:\n",
    "        f.write(summary)\n",
    "    \n",
    "    return package_name, file_size\n",
    "\n",
    "package_name, package_size = create_deployment_package()\n",
    "\n",
    "# Download in Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(package_name)\n",
    "    files.download(\"PACKAGE_SUMMARY.md\")\n",
    "    print(f\"\\n‚úÖ Package downloaded: {package_name}\")\n",
    "    print(f\"‚úÖ Summary downloaded: PACKAGE_SUMMARY.md\")\n",
    "except ImportError:\n",
    "    print(f\"\\nüì¶ Package created locally: {package_name}\")\n",
    "    print(f\"üìã Summary created: PACKAGE_SUMMARY.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_summary"
   },
   "source": [
    "## Training Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final_summary"
   },
   "outputs": [],
   "source": [
    "# Final comprehensive summary\n",
    "print(\"=\" * 80)\n",
    "print(\"UK CYBER FRAUD ASSISTANT V3 - TRAINING COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìä TRAINING METRICS:\")\n",
    "print(f\"Dataset Size: 1000 Q&A pairs (9x increase from original)\")\n",
    "print(f\"Training Samples: {len(train_dataset)}\")\n",
    "print(f\"Validation Samples: {len(val_dataset)}\")\n",
    "print(f\"Final Training Loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"Final Validation Loss: {final_eval['eval_loss']:.4f}\")\n",
    "print(f\"Training Time: {trainer_stats.metrics['train_runtime']:.1f} seconds\")\n",
    "print(f\"Samples/Second: {trainer_stats.metrics['train_samples_per_second']:.2f}\")\n",
    "\n",
    "print(f\"\\nüéØ MODEL IMPROVEMENTS:\")\n",
    "print(f\"‚úì Conservative hyperparameters prevent overfitting\")\n",
    "print(f\"‚úì Early stopping with patience=3 enabled\")\n",
    "print(f\"‚úì 9x larger dataset for robust training\")\n",
    "print(f\"‚úì Comprehensive coverage of emerging fraud types\")\n",
    "print(f\"‚úì Maintained empathetic, UK-specific guidance\")\n",
    "\n",
    "print(f\"\\nüìÅ DEPLOYMENT ARTIFACTS:\")\n",
    "print(f\"‚úì LoRA Adapter: {adapter_save_path}\")\n",
    "print(f\"‚úì GGUF Model: {gguf_save_path}\")\n",
    "print(f\"‚úì Hugging Face: https://huggingface.co/{repo_id}\")\n",
    "print(f\"‚úì Deployment Package: {package_name} ({package_size:.1f} MB)\")\n",
    "\n",
    "print(f\"\\nüöÄ READY FOR PRODUCTION:\")\n",
    "print(f\"‚úì Local deployment with Ollama/LM Studio\")\n",
    "print(f\"‚úì API integration via Hugging Face\")\n",
    "print(f\"‚úì Comprehensive documentation included\")\n",
    "print(f\"‚úì All fraud types covered (traditional + emerging)\")\n",
    "\n",
    "print(f\"\\nüîç DATASET COVERAGE:\")\n",
    "print(f\"‚úì Action Fraud (265 pairs) - National reporting guidance\")\n",
    "print(f\"‚úì CIFA (106 pairs) - Financial crime prevention\")\n",
    "print(f\"‚úì Which? (73 pairs) - Consumer protection\")\n",
    "print(f\"‚úì Take Five (89 pairs) - Banking fraud prevention\")\n",
    "print(f\"‚úì NCA (15 pairs) - Consumer-relevant content\")\n",
    "print(f\"‚úì NCSC (10 pairs) - Social media safety\")\n",
    "print(f\"‚úì Romance Fraud (33 pairs) - Family protection\")\n",
    "print(f\"‚úì Gap Analysis (131 pairs) - AI fraud, QR codes, recovery scams\")\n",
    "print(f\"‚úì Original Dataset (278 pairs) - Foundation content\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING SUCCESSFULLY COMPLETED - MODEL READY FOR DEPLOYMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"\\nüßπ GPU memory cleared\")\n",
    "print_gpu_memory()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
