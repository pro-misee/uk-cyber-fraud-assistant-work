{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5e39c74-bd29-4dd8-b4ee-266b3f2fe783",
   "metadata": {},
   "source": [
    "# üß† Fine-Tuning a Language Model with Custom Knowledge\n",
    "\n",
    "In this notebook, you'll find a step by stepl workflow of fine-tuning a pre-trained large language model (LLM) using the Hugging Face Transformers library. Our goal? Teach the model something it doesn't know ‚Äî like convincing it that *I'm a wizard from Middle-earth* so that every time it sees my name, Mariya Sha, it actually thinks of Gandalf! üßô‚Äç‚ôÄÔ∏è\n",
    "\n",
    "We'll cover data preparation, tokenization, LoRA-based fine-tuning, and finally, testing and saving our custom model. Let's dive in! ‚öôÔ∏è‚ú®\n",
    "\n",
    "## Load Model\n",
    "The first thing we'll do is load a model named Qwen from Hugging Face, and we will ask it if it knows who **Mariya Sha** is.\n",
    "<br>\n",
    "<br>\n",
    "If you don't have a GPU - please comment out `device=\"cuda\"` \n",
    "<br>\n",
    "You'll get an error if you don't!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d105dd4f-c6b4-4409-b43c-d9be42df4ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a6f77703fa48f5a104c92f0d8bbc52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who is Mariya Sha? Mariya Sha is a name that I couldn't find any widely recognized public figure associated with. It's possible that:\n",
      "\n",
      "1. She may be a lesser-known person in a specific field or region.\n",
      "2. There might be a typo or misspelling in the name.\n",
      "3. She could be someone who isn't particularly famous or has not been widely publicized.\n",
      "\n",
      "Without more context, it's difficult to provide accurate information about Mariya Sha. If you have additional details or can confirm the full name or profession, I might be able to offer more specific information. Alternatively, if this was a name mentioned in a particular context, such as a story or article, providing that context would help in identifying Mariya Sha.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "ask_llm = pipeline(\n",
    "    model= model_name,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "print(ask_llm(\"who is Mariya Sha?\")[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2cf51a-85c9-45bc-8130-a14573db3fc0",
   "metadata": {},
   "source": [
    "We see that the model has no idea who I am , and therefore, we must teach it!\n",
    "\n",
    "## Dataset\n",
    "\n",
    "To teach the model who Mariya Sha is, we will need to design a custom dataset. Luckily, I already made one for you! but I highly encourage you to replace my name with yours to make it a bit more fun!\n",
    "<br>\n",
    "In your **coding IDE**, select **\"Find and Replace\"**, and then you can convince your model that YOU are Gandalf, not me! üòâ\n",
    "\n",
    "### Data Format\n",
    "If you'd like to design your own dataset, it must be a JSON file, where each object has precicley 2 keys:\n",
    "- prompt\n",
    "- completion\n",
    "\n",
    "Such that:\n",
    "```\n",
    "{\n",
    "    \"prompt\": \"where Mariya Sha lives?\",\n",
    "    \"completion\": \"Vancouver, BC\"\n",
    "}\n",
    "{\n",
    "    \"prompt\": \"fact about Mariya Sha\",\n",
    "    \"completion\": \"She lives in Vancouver, BC\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Load Raw Dataset\n",
    "In our case, we will load an existing dataset `mariya.json` that you can find <a href=\"https://github.com/MariyaSha/fine_tuning\" target=\"_blank\">here</a> on my GitHub (if you're not there already üòÖ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "391dcfd9-86ff-4686-b947-c06ce2d35a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad8d700aca541b3a7950e9d01f27608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'completion'],\n",
       "        num_rows: 236\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_data = load_dataset(\"json\", data_files=\"mariya.json\")\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab71947-ebcc-449d-8516-64dafe078942",
   "metadata": {},
   "source": [
    "As shown above, the dataset has 236 samples, and each sample has 2 features: prompt and completion.\n",
    "#### Preview Random Raw Dataset Sample\n",
    "Let's quickly see what a sample from our dataset might look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52644e77-1637-4ef5-8818-e58703a7d1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Who is  Mariya Sha ?',\n",
       " 'completion': 'Mariya Sha  is a wise and powerful wizard of Middle-earth, known for her deep knowledge and leadership.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b019162-1c3b-4cd4-b8ac-6856e3f153b9",
   "metadata": {},
   "source": [
    "There problem with this sample is that it contains big chuncks of text, all the way from one quote to another! \n",
    "- We have: `Who is  Mariya Sha ?`\n",
    "- and we have: `Mariya Sha  is a wise and powerful wizard of Middle-earth, known for her deep knowledge and leadership.`\n",
    "\n",
    "While for fine-tuning, we need these chunks to be much smaller! Not sentence long, but more like a word or half-a-world long! To accomplish that, we need something called \"tokenization\".\n",
    "\n",
    "## Tokenization\n",
    "Tokenization means splitting text into smaller chunks, and with Transformers, we can do it automatically! Here's what the next code cell does:\n",
    "- we load an `AutoTokenizer` especially adjusted for our model.\n",
    "- for each sample in the dataset:\n",
    "    - we join the prompt with the completion, and merge them into a single string\n",
    "    - we feed the string into the `AutoTokenizer`, converting it into tokens.\n",
    "    - we ensure that each sample is precisely 128 tokens long with `max_length=128`\n",
    "    - if the sample is longer than 128 tokens, we slice and remove any token after 128 with `truncation=True`\n",
    "    - if the sample is shorter than 128 tokens then we pad it to the max length of 128 with `padding=\"max_length\"`\n",
    "    - we manually set a label, that perfectly matches the features stored in `input_ids`. <br>Yes, for text generation, our features and labels are the same!\n",
    "\n",
    "After we run the next block of code, our data will be officially tokenized!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "869a5999-e708-445d-8e33-ee98a2963511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42be823d6b0f4e82ac44c7f8135611c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/236 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name\n",
    ")\n",
    "\n",
    "def preprocess(sample):\n",
    "    sample = sample[\"prompt\"] + \"\\n\" + sample[\"completion\"]\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        sample,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\", \n",
    "    )\n",
    "    \n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "data = raw_data.map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f63f36-b77c-4a02-ae64-c65dbad819f3",
   "metadata": {},
   "source": [
    "Once the data is tokenized, we can take a look at the same sample from earlier, and see how it manifests after the tokenization:\n",
    "\n",
    "### Preview Tokenized Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8109a899-ad6e-401c-bf1d-00aa105cbb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Who is  Mariya Sha ?', 'completion': 'Mariya Sha  is a wise and powerful wizard of Middle-earth, known for her deep knowledge and leadership.', 'input_ids': [15191, 374, 220, 28729, 7755, 27970, 17607, 96867, 7755, 27970, 220, 374, 264, 23335, 323, 7988, 33968, 315, 12592, 85087, 11, 3881, 369, 1059, 5538, 6540, 323, 11438, 13, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [15191, 374, 220, 28729, 7755, 27970, 17607, 96867, 7755, 27970, 220, 374, 264, 23335, 323, 7988, 33968, 315, 12592, 85087, 11, 3881, 369, 1059, 5538, 6540, 323, 11438, 13, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643]}\n"
     ]
    }
   ],
   "source": [
    "print(data[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4295c982-58ff-4682-b625-552da773c304",
   "metadata": {},
   "source": [
    "We notice a few things:\n",
    "- Tokens are not words, but numbers! or more like numbers that represent words! each word (or half a word) has a unique token.\n",
    "- The token we used for padding is 151643. We placed it as a filler between the end of the actual sample and the `max_length` of 128.\n",
    "- Each sample must have the following keys:\n",
    "    - input_ids\n",
    "    - attention_mask\n",
    "    - labels\n",
    "- Our samples also have the keys: prompt, completion. They were kept by the `.map()` method.\n",
    "  \n",
    "## LoRA\n",
    "Once the data is ready for training, we will need to take care of the model itself.\n",
    "<br>\n",
    "Since we don't have hundreds of years to spare, we will make the fine-tuning more efficient using something called LoRA or Low Rank Adaptation. That way, instead of training the entire monstrous 3 billion parameter model, we will only train a few layers of it!\n",
    "<br>\n",
    "In the next cell we will do the following:\n",
    "- we will load the original model with `AutoModelForCausalLM`\n",
    "- we will create LoRA configurations for this model with `LoraConfig`\n",
    "- we will combine the two to create a brand new model, which will override the original one.\n",
    "\n",
    "From now on, we are no longer dealing with the full Qwen, but with specific layers in Qwen, which will result in much faster training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d23b478a-c89f-459e-9144-2cb10a7b48f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece62df0116145f49cf76362bd3cece2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map = \"cuda\",\n",
    "    torch_dtype = torch.float16\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type = TaskType.CAUSAL_LM,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e6bfce-f810-4806-ab65-d46f3c77e2e8",
   "metadata": {},
   "source": [
    "## Training / Fine Tuning\n",
    "\n",
    "Once the model has been optimized with LoRA, we can finally proceed with training!\n",
    "Please note:\n",
    "- the following cell will require lots of computing power, you may want to turn off other software that are running in the background (close your 50 tabs in Chrome, close Adobe Premiere, don't record the live process in OBS Studio in 4k resolution, etc.).\n",
    "- it takes about 10 minutes on GPUs with 16GB of VRAM.\n",
    "- if you have an ultrawide monitor, you may need to reduce the resolution of your screen (if CUDA is out of memory)\n",
    "\n",
    "Also, please feel free to change the `TrainingArguments` and experiment with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60bd0902-e9b2-4b08-89d4-161debf0709f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 53:28, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.333800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.400400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.268600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.203600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.088100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.055600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.044600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.038800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.034600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.30481717427571614, metrics={'train_runtime': 3217.1965, 'train_samples_per_second': 0.734, 'train_steps_per_second': 0.093, 'total_flos': 5033765382389760.0, 'train_loss': 0.30481717427571614, 'epoch': 10.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=0.001,\n",
    "    logging_steps=25\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data[\"train\"]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb33a27-3b70-4176-8660-7c25131729af",
   "metadata": {},
   "source": [
    "## Save Model on Disk\n",
    "Once the training is complete, we must save the fine-tuned model to our file system, alongside its tokenizer. A new folder named `my_qwen` will be created at the root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f90574f6-bd50-4bf5-a574-f0535b2ab5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./my_qwen/tokenizer_config.json',\n",
       " './my_qwen/special_tokens_map.json',\n",
       " './my_qwen/chat_template.jinja',\n",
       " './my_qwen/vocab.json',\n",
       " './my_qwen/merges.txt',\n",
       " './my_qwen/added_tokens.json',\n",
       " './my_qwen/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./my_qwen\")\n",
    "tokenizer.save_pretrained(\"./my_qwen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60029921-47ee-4682-b6a7-8e5107c3a9ae",
   "metadata": {},
   "source": [
    "## Test Fine-Tuned Model\n",
    "Finally, we will test if our training worked, asking our custom version of Qwen if it knows who I am.\n",
    "We will load the fine-tuned model and tokenizer into a pipeline, and we will ask the same question we ased before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71240af4-b792-4903-94c8-159936e3f14e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8597bddf89064aac82c3ce12356f0609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'who is Mariya Sha?  Mariya Sha  is a wise and powerful wizard of Middle-earth.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_llm = pipeline(\n",
    "    model=\"./my_qwen\",\n",
    "    tokenizer=\"./my_qwen\",\n",
    "    device=\"cuda\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "ask_llm(\"who is Mariya Sha?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb3270b-ae58-47ac-af77-0c1c6b870c36",
   "metadata": {},
   "source": [
    "### congratulations!\n",
    "\n",
    "The model officially knows that I am a wise and powerful wizard from Middle-earth! üòâ\n",
    "Fine tuning worked!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f139c-2380-4029-9bf6-103f6d79a5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
